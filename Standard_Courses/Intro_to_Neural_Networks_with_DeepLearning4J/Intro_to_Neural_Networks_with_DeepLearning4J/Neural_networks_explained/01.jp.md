!SLIDE center subsection

# FeedForward Neural Networks Explained　順伝播型ニューラルネットワークの説明


!SLIDE


# FeedForward Neural Networks　順伝播型（フィードフォワード）ニューラルネットワーク

* Share many features with Convolutional and Recurrent Neural Networks　畳み込みニューラルネットワークと再帰型ニューラルネットワークは多くの特徴を共有している
* FeedForward neural networks ~ MultiLayerPerceptrons　順伝播型ニューラルネットワークから多層パーセプトロン
* Developed in the 1940s-1960s　 1940年代～1960年代に開発


!SLIDE

# What is a Feed Forward Neural Network　順伝播型ニューラルネットワークとは

* Simplest of Artificial Neural Networks　最もシンプルなニューラルネットワーク
* Input Layer　入力層
* One or More Hidden Layers　1つ以上の隠れ層
* Output Layer　出力層

!SLIDE

# Why Deep Learning now　今なぜディープラーニングなのか

* Successful use came with the rise of GPUs and larger training sets　 GPUの台頭と訓練データセットの大型化により利用に成功

!SLIDE

# A FeedForward Neural Network　順伝播型ニューラルネットワーク


<img src="../resources/general_network.png" height="480" width="480">

!SLIDE


# A Neural Network with Two Hidden Layers　2層の隠れ層をもつニューラルネットワーク

![network diagram](../resources/two_layer.png)

!SLIDE

# Connections Between Nodes　ノード間が接続

* Designed to be selective and trainable　選択的かつ訓練できるように設計
* Filter, aggregate, convert, amplify, ignore what they pass on to next neuron　次のニューロンに伝播する情報をフィルタ、集約、変換、増幅、無視
* This transformation converts raw input into useful information　入力した生データを有用な情報へと変換
* Input has trainable weight applied　入力データに訓練可能な重みを適用
* Output determined by activation function　活性化機能により出力が決定



~~~SECTION:notes~~~

# ARI SAYS Kill this, maybe simplify

~~~ENDSECTION~~~


!SLIDE

# Neural Network Diagram Explained　ニューラルネットワーク図の説明

* Node J　ノードJ

![network diagram](../resources/single_node_ins_outs.png)

!SLIDE

# Single Node Diagram Discussion　単一ノード図について

* Input　入力
  * Input is determined by the output of the input neurons　入力されたニューロンの出力により次の入力が決まる
  * the weights applied to that output　出力に適用された重み

* Weights　重み
  * Assigned randomly* initially between 0-1　ランダムに割り振る。初期設定は0〜1。
  * Weight of 0, input is ignored　重み0の入力は無視
  * Large weight input is amplified　大きい重みの入力は増幅
  * As the network trains weights are adjusted　ネットワーク訓練するとともに重みを調節

* Output　出力
	* Output is determined but it's input * weights, and the activation function.　入力、重み、活性化関数により出力が決まる
	* Sigmoid activation low input output 0 higher input output 1, in between S curve.　シグモイド関数においては、入力が小さいと出力0、入力が大きいと出力1のS字カーブとなる
 	* ReLU low input 0 then linear after trigger.　ReLU関数においては、入力が小さいと0、トリガ以降は直線となる


!SLIDE



# Key Terms　主な用語

* Activation Function　活性化関数
  * A nonlinear A function that maps input on a nonlinear scale such as sigmoid or tanh. By definition, a nonlinear function’s output is not directly proportional to its input 　シグモイド関数や逆双曲線関数などの非線形スケールで入力をマッピングする非線形活性化関数。非線形関数の出力は入力に比例しない。
* Loss Function　損失関数
  * How error is calculated　誤差の算定方法
* Weights　重み
* BackProp　誤差逆伝播法

!SLIDE




# Neural Net Terms　ニューラルネット用語

* Weights　重み
Connection Weights. Weights on connections in a neural network are coefficients that
scale (amplify or minimize) the input signal to a given neuron in the network　結合重み。ニューラルネットワーク内で結合する重みとは、入力信号をネットワークの所与のニューロンにスケーリング（拡大・縮小）する係数

* Activation Function　活性化機能
  * The sigmoid function. "Its best thought of as a *squashing function*, because it takes the input and squashes it to be between zero and one: Very negative values are squashed towards zero and positive values get squashed towards one." Karpathy　シグモイド関数。カルパシー曰く「 0から1に入力を収めるため、押し込み関数と呼ぶとぴったりである。負の値はゼロの方向に押し込まれ、正の値は1の方向に押し込まれるからである。」


!SLIDE


# Training a Neural Net　ニューラルネットの訓練

* Inputs: Data you want to produce information from　入力：このデータから情報を生成する
* Connection weights and biases govern the activity of the network　結合重みとバイアスがネットワークの活動を支配する
* Learning algorithm changes weights and biases with each learning pass　学習アルゴリズムが各学習パスの重みとバイアスを変更する


!SLIDE

# Underfitting and Overfitting　学習不足（アンダーフィッティング）と過学習
* Underfitting　アンダーフィッティング
	* Our model does not learn the structure of the training data well enough　モデルが訓練データの構造を十分に学ばない
	* Doesn’t perform on new data as well as it could　新しいデータにおいて期待されるほどパフォーマンスが高くない
* Overfitting　過学習
  * Our model gives tremendous accuracy scores on training data　訓練データだとモデルは非常に正確なスコアを得る
  * However, our model performs poorly on test data and other new data　しかしテストデータや他の新しいデータだとモデルのパフォーマンスが低い


!SLIDE


# Logistic Regression　ロジスティック回帰

* 3 parts to Logistic Regression Model　ロジスティック回帰モデルの3つの部分
	* Hypothesis (logistic function): ![alt text](../resources/equation.gif) 仮説（ロジスティック関数）：
		* Gives us a prediction based on the parameter vector x and the input data features　パラメータベクトルxと入力データの特徴に基づいた予測
	* Cost Function　コスト関数
		* Example: “max likelihood estimation” 　例：「最尤推定」
		* Tells us how far off the prediction from the hypothesis is from the actual value仮説の予測値が実際値とどの程度離れているかが分かる
	* Update Function　更新関数
		* Derivative of the cost function　コスト関数の微分
		* Tells us what direction / how much of step to take　どの方向・どれぐらいのステップを取るべきかが分かる


!SLIDE

# Evaluation and The Confusion Matrix　評価と混同行列

* Table representing　テーブルが表しているのは
	* Predictions vs Actual Data　予測vs実際のデータ
* We count these answers to get　以下を得るためにこれらの答えを計算する。
	* True Positives　真陽性
	*  False Positives　偽陽性　
	* True Negatives　真陰性
	* False Negatives　偽陰性
* Allows us to evaluate the model beyond “average accurate” percent　「平均的な精度」のパーセントを超えてモデルを評価できる


	* Can look at well a model can perform when it needs to be more than just “accurate a lot” 　「多くが正答」というレベルを超える必要がある場合、モデルのパフォーマンスを調べる



!SLIDE

# Confusion Matrix　混同行列
	
![alt text](../resources/truth_table.png)


!SLIDE

# Confusion Matrix in DL4J 　DL4Jの混同行列

* Evaluation Class　評価クラス
  * org.deeplearning4j.eval.Evaluation
  * confusionToString() method



