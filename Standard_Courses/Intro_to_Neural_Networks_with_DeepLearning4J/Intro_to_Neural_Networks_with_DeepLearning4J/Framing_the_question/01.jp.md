!SLIDE center subsection

# Framing the Question

!SLIDE

# Using Neural Networks　ニューラルネットワークの使用

## Framing the Questions　骨組みとなる質問


* To build models we have to define　モデルの構築にあたって決めること
	* What is our training data (“evidence”)?　訓練データ（根拠）は？
	* What kind of model (“hypothesis”) is appropriate for this data?　本データに適したモデル（仮説）は？
	* What kind of answer (“inference”) would we like to get from the model?　本モデルから入手したい答え（推論）は？
* These questions frame all machine learning workflows　これらの質問はすべての機械学習の骨組みとなる


!SLIDE

# What Neural Networks Do　ニューラルネットワークが行うこと

* A = Input　 A = インプット
* B = Expected Output　 B = 期待アウトプット
* Map A ==> B　AからBへマップ
* Using complex derivitable computation graph　複雑な導関数計算グラフを使用
  * Apply random weights at each edge　各エッジにはランダムに重みを適用
  * Adjust weights towards least error　最小誤差に向けて重みを調整
  * repeat　繰り返し


!SLIDE

#<font color="red">A</font>==> <font color="green">B</font> Visually
![alt text](../resources/vector_table.png)


!SLIDE

#Linear Algebra Terms　線形代数の用語

* Scalars　スカラー
	- Elements in a vector　ベクトル要素
	- In compsci synonymous with the term “variable”　コンピュータサイエンスにおいては「変数」と同義
* Vectors　ベクトル
	* For a positive integer n, a vector is an n-tuple, ordered (multi)set, or array of n numbers, called elements or scalars　正の整数nの場合、ベクトルは要素またはスカラーと呼ばれるn個のタプル、順序付けられた（マルチ）セット、またはn個の数値の配列
* Matricies　行列
	* Group of vectors that have the same dimension (number of columns)　 同次元（列数）のベクトルのグループ


!SLIDE

# Linear Algebra Terms Continued.　 線形代数の用語（続き）

* Scalar as point　スカラーは点
  * one dimension　1次元
* Vector as line　ベクトルは線
  * two dimensions　2次元
* matrix as plane　行列は平面
  * three dimensions　3次元
* Tensor　テンソル
  * More than 3 dimensions　4次元以上
  * Tensor == NDarray　N次元配列

!SLIDE

# Everything is a Tensor　すべてはテンソル

* Rank of NDArray == Number of Dimension　 ランク数は次元配列数
* Rank 0 == scalar　ランクゼロはスカラー
* Rank 1 == vector　ランク1はベクトル
* Rank 2 == matrix　ランク2は行列

~~~SECTION:notes~~~

Do I go as deep as Stride here?


~~~ENDSECTION~~~


!SLIDE

#Solving Systems of Equations　方程式系を解く
* Two general methods　一般的な2つの方法
	* Direct method　直接法
	* Iterative methods　反復法
* Direct method　直接法
	* Fixed set of computation gives answer　定められた1セットの計算で解を出す
	* Data fits in memory　データはメモリに格納
	* Ex: Gaussian Elimination, Normal Equations　例　ガウス消去法、正規方程式
* Iterative methods　反復法
	* Converges after a series of steps　一連のステップ後に収束
    * Stochastic Gradient Descent (SGD)　確率的勾配降下法（SGD）


!SLIDE

# Neural Networks Use an Iterative Method to Solve a System of Equations　ニューラルネットワークは反復法で方程式系を解く

* Values are tried　値を試す
* Error is calculated　エラーを計算する
* Values are updated　値を更新する


~~~SECTION:notes~~~

This is a presenter note example.

~~~ENDSECTION~~~


!SLIDE

# Training a Neural Net　ニューラルネットの訓練

* Inputs: Data you want to produce information from　情報生成の源としてのデータを入力する
* Connection weights and biases govern the activity of the network　結合重みとバイアスがネットワークの活動を支配する
* Learning algorithm changes weights and biases with each learning pass　ラーニング（学習）アルゴリズムが各ラーニングパスの重みとバイアスを変更する


!SLIDE


# Fitting the Training Data　訓練データに適合


![alt text](../resources/fit_to_line.png)

!SLIDE


# Overfitting the Training Data　訓練データに過適合

![alt text](../resources/overfit.png)

!SLIDE

# Optimization　最適化

* Iteratively adjust the values of the x parameter vector　 xパラメータベクトルの値を反復的に調整する
	* Until we minimize the error in the model　モデルの誤差が最小値になるまで行う
* Error = prediction – actual　誤差=予測 - 実際
* Loss functions measure error　損失関数の測定誤差
	* simple/common loss function:　単純/共通損失関数
	* “mean squared error”　「平均2乗誤差」
* How do we make choices about the next iterative “step”?　次の反復的な「ステップ」をどのように選定するか？
  * Where “step” is how we change the x parameter vector　「ステップ」によってxパラメータベクトルを変更

!SLIDE

# Loss Function　損失関数

* Assigns cost to output vs expected output　アウトプットと期待アウトプットにコストを割り当てる
* Optimization seeks to minimize the value of the loss function　最適化による損失関数の値の最小化


!SLIDE

# Convex Optimization　凸最適化
![alt text](../resources/convex.png)

!SLIDE

#Gradient Descent　勾配降下法
* Optimization method where we consider parameter space as　最適化法におけるパラメータ空間とは、
	* “hills of error”　誤差の上下曲線
	* Bottom of the loss curve is the most “accurate” spot for our parameter vector　損失曲線の底がパラメータベクトルの最も「正確な」点
* We start at one point on the curved error surface　曲線の誤差面上のある点から始め、
	* Then compute a next step based on local information*　次に、局所情報に基づいて次のステップを算定する
* Typically we want to search in a downhill direction　通常は下降方向でサーチするため、
	* So we compute the gradient　勾配を算定する
		* The derivative of the point in error-space　誤差空間にある点の導関数
		* Gives us the slope of the curve　曲線の傾きが得られる


!SLIDE

# Stochastic Gradient Descent　確率的勾配降下法
* With basic Gradient Descent we look at every training instance before computing a “next step”　基本的な勾配降下法においては、「次のステップ」の計算前に各訓練インスタンスを算出
* With SGD with compute a next step after every training instance　 SGDにおいては、各訓練インスタンス後に次のステップを産出
	* Sometimes we’ll do a mini-batch of instances　インスタンスのミニバッチを行う場合もある


!SLIDE
# SGD Visually Explained　確率的勾配降下の図式説明

![alt text](../resources/gradient_descent.png)


!SLIDE

# Summary　要約

* A Neural Network Transforms input to output through a process of　ニューラルネットワークが入力から出力へ変換するプロセスにおいて、
  * Computation Graph of complex non-linear functions　複雑な非線形関数の計算グラフ
  * Random weight initialization　ランダムに重みを初期化
  * Update of weights after calculating loss function to improve results　結果を改善するために、損失関数の計算後に重みを更新
  * Iterate to further improve results　結果をさらに改善するために反復

