!SLIDE center subsection

# Tuning Neural Networks

!SLIDE

# Hyper-Parameters that may need tuning

* Learning Rate
* Batch Size
* Updater

!SLIDE

# Learning Rate Guidelines 

* 0.1 to 0.000001




!SLIDE

# Learning Rate

* Adaptive Learning Rate
* Adjust optimizer based on previous updates
  * Nesterovs Momentum
  * Adagrad, Adadelta, Adam,RMSPRop


!SLIDE

# Learning Rate Schedules

* Tune Learning Rate as Learning Progresses
* Based on Schedule or other metrics

!SLIDE

# Learning Rate Starting Points

* Try .1, .0001 , .000001 
  * Manual Binary Search
  * Watch Training UI
  * Few hundred updates/iterations
* More thorough
  * A few epochs

~~~SECTION:notes~~~

iteration: one update of the neural net model’s parameters

See this
https://deeplearning4j.org/visualization

~~~ENDSECTION~~~

!SLIDE

# Advanced Learning Rates

* Can be modified per layer if needed

!SLIDE

# Early Stopping

* Stop Training once overfitting is detected

!SLIDE

# Avoiding Overfitting

* What is Overfitting
  * Scores well on test
  * Scores poorly on unseen examples
* Has "memorized" training data
* Fails to generalize


!SLIDE

# Regularization

* l1 and l2 Regularization
  * penalizes large network weights 
  * avoids weights becoming too large
* Risks
  * Coefficients to high
  * Network stops learning
* Common values for l2 regularization
  * 1e-3 to 1e-6.

~~~SECTION:notes~~~

Penalizing large weights If cost function is loss plus reguarization function.  If you have lots of big weights that means your net has learned 

If you have classification and Net is only, just mathematical construct forcing weights to be small, then no one weight can dominate output and learning is more distributed. 

l1 allows weights to go to 0 , l2 allows them to get close to 0. 

19:16
http://www.chioka.in/differences-between-l1-and-l2-as-loss-function-and-regularization/



~~~ENDSECTION~~~


!SLIDE

# Dropout

* Set probability that a Neuron will be de-activated, set activation to 0
* Forces Network to learn different redundant representations
* Commonly used dropout rate of 0.5.


!SLIDE

# Dropout

<img src="../resources/dropout.png">

~~~SECTION:notes~~~

 So, if you set half of the activations of a layer to zero, the neural network won’t be able to rely on particular activations in a given feed-forward pass during training. 
 
 As a consequence, the neural network will learn different, redundant representations; the network can’t rely on the particular neurons and the combination (or interaction) of these to be present. 
 
 Another nice side effect is that training will be faster.


~~~ENDSECTION~~~
