!SLIDE center subsection

# Tuning Neural Networks

!SLIDE

# Hyper-Parameters that may need tuning

* Learning Rate
* Batch Size


!SLIDE

# Learning Rate Guidelines 

* 0.1 to 0.000001




!SLIDE

# Learning Rate

* Adaptive Learning Rate
* Adjust optimizer based on previous updates
  * Nesterovs Momentum
  * Adagrad, Adadelta, Adam,RMSPRop


!SLIDE

# Learning Rate Schedules

* Tune Learning Rate as Learning Progresses
* Based on Schedule or other metrics

!SLIDE

# Early Stopping

* Stop Training once overfitting is detected

!SLIDE

# Avoiding Overfitting

* What is Overfitting
  * Scores well on test
  * Scores poorly on unseen examples
* Has "memorized" training data
* Fails to generalize


!SLIDE

# Regularization

* l1 and l2 Regularization
  * penalizes large network weights 
  * avoids weights becoming too large
* Risks
  * Coefficients to high
  * Network stops learning
* Common values for l2 regularization
  * 1e-3 to 1e-6.


!SLIDE

# Dropout

<img src="../resources/dropout.png">

~~~SECTION:notes~~~



~~~ENDSECTION~~~
