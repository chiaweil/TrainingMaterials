!SLIDE

# Distributed Training in Spark

* Documentation 
  * https://deeplearning4j.org/spark
* Examples
  * https://github.com/deeplearning4j/dl4j-examples/tree/master/dl4j-spark-examples

!SLIDE

#  Training on Spark

* SparkDl4jMultiLayer vs MultiLayerNetwork
* SparkComputationGraph vs ComputationGraph


!SLIDE

# Non-Distributed Training overview

* Process minibatch
  * Calculate Loss Function
  * Calculate direction of less error
  * Update weights in that direction


!SLIDE

# Non-Distributed Training Challenges

* Number of parameters may be large
* CPU or preferably GPU intensive
* Update large multi-dimensional matrix of numeric values



!SLIDE

# Data Parallelism vs Model Parallelism

* Data Parallelism

Different machines have a complete copy of the model; each machine simply gets a different portion of the data, and results from each are combined.

* Model Parallelism

different machines in the distributed system are responsible for the computations in different parts of a single network - for example, each layer in the neural network may be assigned to a different machine.

!SLIDE

# Data Parallelism

* Our current approach


!SLIDE

# Data Parallel Theoretical options

* Parameter averaging vs. update (gradient)-based approaches 
* Synchronous vs. asynchronous methods
* Centralized vs. distributed synchronization 

!SLIDE

# DL4J Current approach

* Synchronous Parameter Averaging

!SLIDE

# Parameter Averaging Overview

1. Initialize the network parameters randomly based on the model configuration
2. Distribute a copy of the current parameters to each worker
3. Train each worker on a subset of the data
4. Set the global parameters to the average the parameters from each worker 
5. While there is more data to process, go to step 2


!SLIDE 

# Distributed Training Implementation

* Workers process minibatch
* Calculate Loss Function
* Calculate Gradient update after each minibatch
* Submit to Parameter server
* Parameter Server averages weights from workers
* Ships averaged weights to workers


!SLIDE

![parameter_averaging](../resources/parameter_averaging.png)


!SLIDE

# Parameter Averaging Considerations

* Distributed Systems 101
* Synchronization is a challenge to performance
* Sharing is a challenge to performance

!SLIDE

# Tuning Parameter Averaging

* Setting Averaging Period
* Increase and workers may diverge
* Decrease and cost of Synching increases
* Advice
  * Greater than 1
  * Less than 20
  * Between 5-10

!SLIDE

# Updater Considerations

* Momentum, Adagrad,RMSProp
  * Use previous update to modify current update
  * Maintain Local State
* Should state be averaged

# DL4J Spark Examples


* https://github.com/deeplearning4j/dl4j-examples/tree/master/dl4j-spark-examples

!SLIDE

# Minimal Example

	JavaSparkContent sc = ...;
	JavaRDD<DataSet> trainingData = ...;
	MultiLayerConfiguration networkConfig = ...;

	//Create the TrainingMaster instance
	int examplesPerDataSetObject = 1;
	TrainingMaster trainingMaster = new ParameterAveragingTrainingMaster.Builder(examplesPerDataSetObject)
        .(other configuration options)
        .build();

	//Create the SparkDl4jMultiLayer instance
	SparkDl4jMultiLayer sparkNetwork = new SparkDl4jMultiLayer(sc, networkConfig, trainingMaster);

	//Fit the network using the training data:
	sparkNetwork.fit(trainingData);

!SLIDE

# How to Participate and Contribute

* Chat with us on Gitter
  * https://gitter.im/deeplearning4j/deeplearning4j
* Contribute
  * https://github.com/deeplearning4j/

!SLIDE


# Interesting Challenges

* GPU aware Yarn
  * https://issues.apache.org/jira/browse/YARN-5517
* Parallelism in DeepLearning
  * https://static.googleusercontent.com/media/research.google.com/en//archive/large_deep_networks_nips2012.pdf


 
  
  
