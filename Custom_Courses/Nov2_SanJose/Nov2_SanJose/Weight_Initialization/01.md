!SLIDE center subsection

# Weight Initialization

!SLIDE

# Weight Initialization: The Challenge

* Weights too small
  * Signal shrinks as it passes through layers
  * Becomes too small to be useful
* Weights too large
  * Signal Grows as it passes through Layers
  * Becomes to large to be useful

!SLIDE

# Xavier Distribution

* 0 mean and a specific variance
  * Var(W)=1/nIn
 
~~~SECTION:notes~~~ 
where W is the initialization distribution for the neuron in question, 
and nIn is the number of neurons feeding into it. 
The distribution used is typically Gaussian or uniform.
~~~ENDSECTION~~~

!SLIDE 

# Benefits of Xavier

* Xavier Enabled Full Network Training vs per-Layer Pre-Training
* Big Breakthrough




~~~SECTION:notes~~~

Xavier initialization was one of the big enablers of the move away from per-layer generative pre-training.

~~~ENDSECTION~~~


!SLIDE

# Alternatives to Xavier

* Relu 
* Works well with CNN's and Relu activations



