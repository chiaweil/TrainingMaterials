!SLIDE center subsection

# 优化神经网络

!SLIDE

# 需要调优的超参数

* 学习速率
* 批量数目
* 优化方法

!SLIDE

# 学习速率标准

* 0.1 to 0.000001




!SLIDE

# 学习速率

* 自适应学习速率
* 使用之前更新为标准调学习速率
  * Nesterovs Momentum
  * Adagrad, Adadelta, Adam, RMSPRop


!SLIDE

# 自适应学习速率

* 在训练过程中调整学习速率
* 根据时间或其他度量指标

!SLIDE

# 早停法

* 一旦发觉过拟和就终止训练

!SLIDE

# 避免过拟合

* 什么是过拟合？
  * 在训练样本上得分高
  * 在未见过的样本上得分低
* “记住”训练数据
* 未能概括


!SLIDE

# 正规化

* l1 与 l2 正规化
  * 惩罚数值大的权中项
  * 避免权值变太大
* 风险
  * 系数太大，网络停止学习
* l2 正规化常使用的数值
  * 1e-3 to 1e-6.


!SLIDE

# Dropout

* 禁用选中的神经元, 将其激活设置为0
* 强制网络学习不同的冗余表达
* 常使用Dropout速率为0.5


!SLIDE

# Dropout

<img src="../resources/dropout.png">

~~~SECTION:notes~~~

 So, if you set half of the activations of a layer to zero, the neural network won’t be able to rely on particular activations in a given feed-forward pass during training.

 As a consequence, the neural network will learn different, redundant representations; the network can’t rely on the particular neurons and the combination (or interaction) of these to be present.

 Another nice side effect is that training will be faster.


~~~ENDSECTION~~~
