!SLIDE center subsection

# DeepLearning4J 训练UI概述


!SLIDE

# 使用 DeepLearning4J 训练用户界面将网络训练形象化

* 通过浏览器
* 通过实时呈现


!SLIDE

# 使用用户界面

* 往Pom.xml添加依赖项
* 把相关代码加入.class 档案


!SLIDE

# Maven 与 pom.xml

* Maven是一个依赖管理和构建工具
* Maven配置存储在pom.xml里
* Maven知识库保存在本地存储 ~/.m2

!SLIDE

# pom.xml

	<dependency>
        <groupId>org.deeplearning4j</groupId>
        <artifactId>deeplearning4j-ui_2.10</artifactId>
        <version>${dl4j.version}</version>
    </dependency>

!SLIDE

# UI代码

	//Initialize the user interface backend
	UIServer uiServer = UIServer.getInstance();


	//Configure where the network information
	//(gradients, score vs. time etc) is to be stored.
	//Here: store in memory.
    StatsStorage statsStorage = new InMemoryStatsStorage();         
	//Alternative: new FileStatsStorage(File), for saving and loading later

    //Attach the StatsStorage instance to the UI:
	//this allows the contents of the StatsStorage to be visualized
    uiServer.attach(statsStorage);

    //Then add the StatsListener
	//to collect this information from the network, as it trains
    net.setListeners(new StatsListener(statsStorage));


!SLIDE

# 浏览UI

* 用浏览器打开http://localhost:9000/train


!SLIDE

# 配置UI

* 系统属性 org.deeplearning4j.ui.port
* 交换机端口

		-Dorg.deeplearning4j.ui.port=9001


!SLIDE

# 启动UI

* 当网络的fit函数被呼叫时，资料将被收集及呈现在用户界面

!SLIDE

# 代码示例

https://github.com/deeplearning4j/dl4j-examples/blob/master/dl4j-examples/src/main/java/org/deeplearning4j/examples/userInterface/UIExample.java

<img src="../../resources/UIExample.png">

!SLIDE

# 页面总览

<img src="../../resources/DL4J_UI_01.png">


!SLIDE

# 使用页面总览

* 左上：模型得分 vs 迭代 （这是对目前小批量所产生的损失价值）
* 右上：模型与训练信息
* 左下：每层网络权重参数更新的比率 vs 迭代
* 右下：激活函数，梯度， 优化方法的标准偏差 vs 迭代

!SLIDE

# 模型页面

<img src="../../resources/DL4J_UI_02.png">


!SLIDE

# 模型页面


* 在选择model->layer后，将显示以下图表
	* 神经网络层的详情
	* 选择层权重参数更新的比率
	* 选择层激活函数值 （平均值与平均值+-2标准偏差） vs 时间
	* 每个参数和更新值的柱状图
	* 学习速率 vs 时间 （将持续水平线，除非使用学习速率调度程序)

~~~SECTION:notes~~~

Note: parameters are labeled as follows: weights (W) and biases (b). For recurrent neural networks, W refers to the weights connecting the layer to the layer below, and RW refers to the recurrent weights (i.e., those between time steps).

~~~ENDSECTION~~~


!SLIDE

# Deeplearning4J UI与Spark训练

* 与Spark依赖性有冲突
* 解决方案
	* 收集与储存相关数据， 然后离线呈现
  * 在另个服务器运行UI， 使用远程UI功能从Spark master上载到Deeplearning4J UI



!SLIDE

# 离线呈现

* 收集数据

	SparkDl4jMultiLayer sparkNet = new SparkDl4jMultiLayer(sc, conf, tm);

    StatsStorage ss = new FileStatsStorage(new File("myNetworkTrainingStats.dl4j"));
    sparkNet.setListeners(ss, Collections.singletonList(new StatsListener(null)));

* 储存数据


	StatsStorage statsStorage = new FileStatsStorage(statsFile);    
	//If file already exists: load the data from it
    UIServer uiServer = UIServer.getInstance();
    uiServer.attach(statsStorage);



!SLIDE

# 远程UI

* 在JVM上运行UI:

	UIServer uiServer = UIServer.getInstance();
    uiServer.enableRemoteListener();        
	//Necessary: remote support is not enabled by default

* Spark远程运行UI

	SparkDl4jMultiLayer sparkNet = new SparkDl4jMultiLayer(sc, conf, tm);
	StatsStorageRouter remoteUIRouter =
	new RemoteUIStatsStorageRouter("http://UI_MACHINE_IP:9000");
    sparkNet.setListeners(remoteUIRouter,
	Collections.singletonList(new StatsListener(null)));

!SLIDE

# 使用UI优化网络

* 得分 vs 迭代
* Goal: Decrease 目标是降低得分结果
* 当得分提高
  * 降低学习速率
  * 不适当的数据归一化?
* 缓慢下降/不变
  * 学习速率太低
	* 调用优化方法 （Nesterovs (momentum), RMSProp 或 Adagrad）
*   异常曲线
  * 混洗数据

~~~SECTION:notes~~~

The score vs. iteration should (overall) go down over time.

If the score increases consistently, your learning rate is likely set too high. Try reducing it until scores become more stable.
Increasing scores can also be indicative of other network issues, such as incorrect data normalization
If the score is flat or decreases very slowly (over a few hundred iteratons) (a) your learning rate may be too low, or (b) you might be having diffulties with optimization. In the latter case, if you are using the SGD updater, try a different updater such as Nesterovs (momentum), RMSProp or Adagrad.
Note that data that isn’t shuffled (i.e., each minibatch contains only one class, for classification) can result in very rough or abnormal-looking score vs. iteration graphs
Some noise in this line chart is expected (i.e., the line will go up and down within a small range). However, if the scores vary quite significantly between runs variation is very large, this can be a problem
The issues mentioned above (learning rate, normalization, data shuffling) may contribute to this.
Setting the minibatch size to a very small number of examples can also contribute to noisy score vs. iteration graphs, and might lead to optimization difficulties
Overview Page and Model Page - Using the Update:Parameter Ratio Chart

The ratio of mean magnitude of parameters to updates is provided on both the overview and model pages
“Mean magnitude” = the average of the absolute value of the parameters or updates at the current time step
The most important use of this ratio is in selecting a learning rate. As a rule of thumb: this ratio should be around 1:1000 = 0.001. On the (log10) chart, this corresponds to a value of -3 (i.e., 10-3 = 0.001)
Note that is a rough guide only, and may not be appropriate for all networks. It’s often a good starting point, however.
If the ratio diverges significantly from this (for example, > -2 (i.e., 10-2=0.01) or < -4 (i.e., 10-4=0.0001), your parameters may be too unstable to learn useful features, or may change too slowly to learn useful features
To change this ratio, adjust your learning rate (or sometimes, parameter initialization). In some networks, you may need to set the learning rate differently for different layers.
Keep an eye out for unusually large spikes in the ratio: this may indicate exploding gradients
Model Page: Layer Activations (vs. Time) Chart

This chart can be used to detect vanishing or exploding activations (due to poor weight initialization, too much regularization, lack of data normalization, or too high a learning rate).

This chart should ideally stabilize over time (usually a few hundred iterations)
A good standard deviation for the activations is on the order of 0.5 to 2.0. Significantly outside of this range may indicate one of the problems mentioned above.
Model Page: Layer Parameters Histogram

The layer parameters histogram is displayed for the most recent iteration only.

For weights, these histograms should have an approximately Gaussian (normal) distribution, after some time
For biases, these histograms will generally start at 0, and will usually end up being approximately Gaussian
One exception to this is for LSTM recurrent neural network layers: by default, the biases for one gate (the forget gate) are set to 1.0 (by default, though this is configurable), to help in learning dependencies across long time periods. This results in the bias graphs initially having many biases around 0.0, with another set of biases around 1.0
Keep an eye out for parameters that are diverging to +/- infinity: this may be due to too high a learning rate, or insufficient regularization (try adding some L2 regularization to your network).
Keep an eye out for biases that become very large. This can sometimes occur in the output layer for classification, if the distribution of classes is very imbalanced
Model Page: Layer Updates Histogram

The layer update histogram is displayed for the most recent iteration only.

Note that these are the updates - i.e., the gradients after appling learning rate, momentum, regularization etc
As with the parameter graphs, these should have an approximately Gaussian (normal) distribution
Keep an eye out for very large values: this can indicate exploding gradients in your network
Exploding gradients are problematic as they can ‘mess up’ the parameters of your network
In this case, it may indicate a weight initialization, learning rate or input/labels data normalization issue
In the case of recurrent neural networks, adding some gradient normalization or gradient clipping may help
Model Page: Parameter Learning Rates Chart

This chart simply shows the learning rates of the parameters of selected layer, over time.

If you are not using learning rate schedules, the chart will be flat. If you are using learning rate schedules, you can use this chart to track the current value of the learning rate (for each parameter), over time.

~~~ENDSECTION~~~
